# EASS Auto-Grader Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# AI PROVIDER SELECTION
# =============================================================================
# Available providers:
#   gemini-cli   - Gemini CLI tool (uses OAuth, no API key needed)
#   gemini-api   - Direct Gemini API calls (requires GEMINI_API_KEY)
#   codex        - OpenAI Codex CLI
#   local        - Local LLM via Ollama/llama-server
#
# Default: gemini-cli
AI_PROVIDER=gemini-cli

# =============================================================================
# GEMINI CLI (local tool, uses OAuth - no API key required)
# =============================================================================
# The Gemini CLI authenticates via browser OAuth flow
# Just run 'gemini' and follow the prompts to authenticate
# Model to use with Gemini CLI
GEMINI_CLI_MODEL=gemini-2.5-flash

# =============================================================================
# GEMINI API (direct HTTP calls - requires API key)
# =============================================================================
# Get your API key from: https://aistudio.google.com/apikey
# Use this for: curl, PydanticAI, direct API integrations
# Free tier: 100 requests/day with Gemini 2.5 Pro
GEMINI_API_KEY=
GEMINI_API_MODEL=gemini-2.5-flash
# API endpoint (default is Google AI Studio)
GEMINI_API_ENDPOINT=https://generativelanguage.googleapis.com/v1beta

# =============================================================================
# CODEX CLI (OpenAI)
# =============================================================================
# Uses your OpenAI credentials (from ~/.codex/config.toml or OPENAI_API_KEY)
# Available models: o4-mini, o3, gpt-4.1, codex
CODEX_MODEL=o4-mini

# =============================================================================
# LOCAL LLM (Ollama/llama-server)
# =============================================================================
# For offline/self-hosted inference
# Server URL (OpenAI-compatible API)
LOCAL_LLM_URL=http://127.0.0.1:1234
# Context window size
LOCAL_LLM_CONTEXT_SIZE=8192
# Model name (if using Ollama)
LOCAL_LLM_MODEL=gemma3:27b
# HuggingFace model for llama-server
LOCAL_LLM_HF_MODEL=ggml-org/gemma-3-27b-it-GGUF
# Auto-start local LLM if not running (true/false)
AUTO_START_LOCAL_LLM=false

# =============================================================================
# EVALUATION SETTINGS
# =============================================================================
# Timeout per AI request in seconds
AI_TIMEOUT=300

# Number of parallel workers for batch evaluation
WORKERS=4

# Fallback chain: if primary provider fails (quota, rate limit, error), try this
# Options: codex, gemini-cli, gemini-api, local
# Examples:
#   AI_FALLBACK_CHAIN=codex      # Primary=gemini → fallback to Codex
#   AI_FALLBACK_CHAIN=gemini-api # Primary=codex → fallback to Gemini API
#   AI_FALLBACK_CHAIN=local      # Fallback to local LLM
AI_FALLBACK_CHAIN=codex
